{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings from Wikipedia\n",
    "\n",
    "Train a simple embedding model by calling random Wikipedia article (abstracts only). This is to practise training on streamline data.\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Extract Context and Label\n",
    "3. Dynamically growing Vocabulary\n",
    "4. Define a CBow model\n",
    "5. Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 Data Cleaning\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove non characters\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    # remove duplicate white spaces\n",
    "    text = \" \".join(text.split())\n",
    "    # remove citations\n",
    "    text = re.sub(\"\\[\\d*\\]\", \"\", text)\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "  \n",
    "def get_random_wiki():\n",
    "    # get random article\n",
    "    URL = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "    page = requests.get(URL)\n",
    "    target_url = page.request.url\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # extract intro\n",
    "    texts = []\n",
    "    for o in soup.find_all(class_=\"mw-parser-output\"):\n",
    "        texts.extend([s.text for s in o.find_all(\"p\")])\n",
    "        texts = [clean_text(t) for t in texts if t not in [\"\\n\", \"\"]]\n",
    "    return texts, target_url\n",
    "\n",
    "\n",
    "### 2 Extract Context and Label\n",
    "def get_training(corpus, window_size, pad_token=0):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    ws = 2 * window_size\n",
    "    for sentence in corpus:\n",
    "        n = len(sentence)\n",
    "        for i, word in enumerate(sentence):\n",
    "            # get indices of context\n",
    "            choose = [i + k for k in range(-window_size, window_size + 1) if\n",
    "                    k != 0 and i + k >= 0 and i + k < n]\n",
    "            # choose elements\n",
    "            context = tf.gather(sentence, choose)\n",
    "            context = tf.concat([context, [pad_token] * (ws - len(choose))], axis=0)  # pad with 0\n",
    "            y = word\n",
    "            inputs.append(context)\n",
    "            labels.append(y)\n",
    "    return tf.stack(inputs), tf.stack(labels)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "### 3. Dynamically increasing Vocabulary\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Have a constantly learning Vocabulary. Stores key-value and value-dict dictionaries, adds new elements\n",
    "    to the dictionary and also acts as generator for Wikipedia abstract training data. Includes a word counter.\n",
    "    :param max_size: Maximum size of Vocabulary. (Idea: delete rare words if too full)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size = 300000):\n",
    "        self.pad_id = 0 \n",
    "        self.word_index = 1\n",
    "        self.vocab = {}\n",
    "        self.vocab_inv = {}\n",
    "        self.word_counts = defaultdict(lambda: 0)\n",
    "        self.max_size = max_size\n",
    "        self.full_dict_warning = False # notify if dict is full\n",
    "\n",
    "    def add_key(self, key):\n",
    "        # add a new key if vocabulary not yet full\n",
    "        if self.word_index > self.max_size and not self.full_dict_warning:\n",
    "            print(\"ATTENTION: Dictionary is full - no more words are added\")\n",
    "            self.full_dict_warning = True\n",
    "        else:\n",
    "            self.vocab[key] = self.word_index\n",
    "            self.vocab_inv[self.word_index] = key\n",
    "            self.word_index += 1\n",
    "\n",
    "    def get_one(self, key):\n",
    "        # input a word and return the corresponding integer token\n",
    "        if key not in self.vocab:\n",
    "            self.add_key(key)\n",
    "        self.word_counts[key] += 1    \n",
    "        return self.vocab[key]\n",
    "\n",
    "    def get_sentence(self, sentence):\n",
    "        # input a sentence (list of words) and return a list of integer tokens\n",
    "        return [self.get_one(key) for key in sentence.split()]\n",
    "\n",
    "    def get(self, text):\n",
    "        # input list of sentences and return list of list of integer tokens\n",
    "        return [self.get_sentence(sentence) for sentence in text]\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        # return entire vocabulary\n",
    "        return list(self.vocab.keys())\n",
    "\n",
    "    def get_training_data(self, size=100, n_articles=5, window_size=2, pad_token=0):\n",
    "        # yield tuples of labels and context\n",
    "        # n_articles: how man articles to yield at a time\n",
    "        # window_size: size of context\n",
    "        for j in range(size):\n",
    "            texts = []\n",
    "            urls = []\n",
    "            for i in range(n_articles):\n",
    "                text, url = get_random_wiki()\n",
    "                texts.extend(text)\n",
    "                urls.append(url)\n",
    "            texts_encoded = self.get(texts)\n",
    "            yield get_training(corpus=texts_encoded, window_size=window_size, pad_token=pad_token), urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBow model\n",
    "\n",
    "Implement a CBow model as in the *Embeddings_Low_Level.ipynb* but use keras implementation of specific layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a shorter word2vec version than before\n",
    "class CbowCustom(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Simplec Cbow model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CbowCustom, self).__init__()\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.soft = tf.keras.layers.Dense(vocab_size-1, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embed(inputs)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.soft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximum vocabulary and embedding size\n",
    "vocab_size = 300000\n",
    "embed_size = 25\n",
    "# create a vocabulary instance\n",
    "V = Vocab(max_size=vocab_size)\n",
    "# create a CBow instance\n",
    "m = CbowCustom(vocab_size=vocab_size, embed_size=embed_size)\n",
    "# combile\n",
    "m.compile(optimizer=\"rmsprop\", loss=tf.keras.losses.sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Use Vocabulary to create training data and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5\n",
      "398/398 [==============================] - 67s 165ms/step - loss: 12.4428\n",
      "2/5\n",
      "22/22 [==============================] - 3s 145ms/step - loss: 12.1460\n",
      "3/5\n",
      "213/213 [==============================] - 35s 163ms/step - loss: 11.7104\n",
      "4/5\n",
      "97/97 [==============================] - 18s 185ms/step - loss: 11.1156\n",
      "5/5\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 10.9084\n"
     ]
    }
   ],
   "source": [
    "# store all wikipedia articles used for training\n",
    "urls = []\n",
    "# Parameter to choose number of \"epochs\"\n",
    "k = 5\n",
    "\n",
    "# create a generator\n",
    "gen = V.get_training_data(size=k, n_articles=2)\n",
    "for i in range(k):\n",
    "    # progress bar\n",
    "    print(f\"\\r{i+1}/{k}\")\n",
    "    # try - sometimes errors in cleaning\n",
    "    try:\n",
    "        temp, urls = next(gen)\n",
    "        urls.extend(urls)\n",
    "        # train step\n",
    "        m.fit(*temp, batch_size=10)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "Check if similar words are more similar than very different words. Seems legit :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "england-new: 0.5517\n",
      "england-cricket: 0.5174\n",
      "england-small: 0.2995\n",
      "england-convincing: 0.278\n"
     ]
    }
   ],
   "source": [
    "def cor(x, y):\n",
    "    # simple pearson correlation\n",
    "    xc = tf.math.square(x - tf.reduce_mean(x))\n",
    "    yc = tf.math.square(y - tf.reduce_mean(y))\n",
    "    nom = tf.reduce_sum(tf.multiply(xc, yc))\n",
    "    denom = tf.math.reduce_std(x) * tf.math.reduce_std(y)\n",
    "    return float(nom/denom)\n",
    "\n",
    "def compare_words(x,y):\n",
    "    print(f\"{x}-{y}: {round(cor(m.embed(V.vocab[x]), m.embed(V.vocab[y])), 4)}\")\n",
    "\n",
    "compare_words(\"england\", \"new\")\n",
    "compare_words(\"england\", \"cricket\")\n",
    "compare_words(\"england\", \"small\")\n",
    "compare_words(\"england\", \"convincing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
