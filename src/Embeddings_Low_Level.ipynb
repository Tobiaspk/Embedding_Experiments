{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practise Embeddings with custom layers\n",
    "\n",
    "*Inspiration*\n",
    "\n",
    "- [original paper](https://arxiv.org/abs/1301.3781)\n",
    "- [easy implementation](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html#:~:text=(%2018%3An14%20)-,Implementing%20Deep%20Learning%20Methods%20and%20Feature%20Engineering%20for%20Text%20Data,Continuous%20Bag%20of%20Words%20(CBOW)&text=The%20CBOW%20model%20architecture%20tries,Science%20Lead%20at%20Applied%20Materials.)\n",
    "\n",
    "## Ideas improvement\n",
    "\n",
    "- Do not train on common \"words\". i.e. Use a word as label only if a scaled runif number is < tfidf(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# model\n",
    "# 1. EmbedCustom: embedding\n",
    "# 2. AvgInputsCustom: average them\n",
    "# 3. SoftCustom: Apply linear model Softmax\n",
    "\n",
    "class EmbedCustom(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Embedding layer. Calls specific slices of the weights instead of applying matrix multiplication.\n",
    "    :param vocab_size: integer defining the desired size of the weights\n",
    "    :param embed_size: integer describing the size of the embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_size: int):\n",
    "        super(EmbedCustom, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def build(self, input_shape: tuple):\n",
    "        # initialised weight on first call of object\n",
    "        self.W = self.add_weight(shape=(self.vocab_size, self.embed_size),\n",
    "                                 trainable=True,\n",
    "                                 initializer=\"random_normal\") # input_shape[1] -> vocab_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # return the weights with respect to an array/list of indices\n",
    "        return tf.gather(self.W, inputs)\n",
    "\n",
    "class AvgInputsCustom(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Averaging layer takes as input an (n, m, k) array and averages it along the 1st axis to output an array of dimension (n, k)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AvgInputsCustom, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_mean(inputs, axis=1)\n",
    "\n",
    "class SoftCustom(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Dense layer with Softmax activation to regress the embedding to an vocabulary size.\n",
    "    :param vocab_size: Size of vocabulary (output size)\n",
    "    :param embed_size: Size of the embedding (input size)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SoftCustom, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(embed_size, vocab_size),\n",
    "                                 trainable=True,\n",
    "                                 initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(1, vocab_size),\n",
    "                                 trainable=True,\n",
    "                                 initializer=\"random_normal\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.softmax(tf.add(tf.matmul(inputs, self.W), self.b))\n",
    "\n",
    "class EmbedModel(tf.keras.models.Model):\n",
    "    \"\"\"\n",
    "    Embedding model combining all layers defined above into a keras model.\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param embed_size: Size of the embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbedModel, self).__init__()\n",
    "        self.embed_layer = EmbedCustom(vocab_size, embed_size)\n",
    "        self.avg_layer = AvgInputsCustom()\n",
    "        self.soft_layer = SoftCustom(vocab_size, embed_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embed_layer(inputs)\n",
    "        x = self.avg_layer(x)\n",
    "        x = self.soft_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on generated tokens\n",
    "\n",
    "Instead of using real sentences, run the model on some randomly generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"data\" as random uniform numbers, representing fixed sentence lengths of 10\n",
    "a = tf.random.uniform(shape=(300, 10), minval=0, maxval=10, dtype=tf.int64)\n",
    "# define vocab size + 1 because of paddingss token\n",
    "vocab_size = len(tf.unique(tf.reshape(a, (-1)))[0]) + 1\n",
    "pad_token = vocab_size - 1\n",
    "# define size of embedding\n",
    "embed_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training(corpus, window_size):\n",
    "    \"\"\"\n",
    "    Get all possible training tuples out of a corpus and return (inputs, label) tuples\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    ws = 2 * window_size\n",
    "    # extract all tuples from each sentence\n",
    "    for sentence in corpus:\n",
    "        n = len(sentence)\n",
    "        # use each word once for \n",
    "        for i, word in enumerate(sentence):\n",
    "            # get ids of context\n",
    "            choose = [i + k for k in range(-window_size, window_size + 1) if\n",
    "                        k != 0 and i + k >= 0 and i + k < n]\n",
    "            # choose elements\n",
    "            context = tf.gather(sentence, choose)  \n",
    "            context = tf.concat([context, [pad_token] * (ws - len(choose))], axis=0)  # pad with 0\n",
    "            y = tf.keras.utils.to_categorical(word, vocab_size)\n",
    "            inputs.append(context)\n",
    "            labels.append(y)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = EmbedModel(vocab_size + 1, embed_size)\n",
    "m.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# get training data\n",
    "inputs, labels = get_training(a, 2)\n",
    "inputs = tf.stack(inputs)\n",
    "labels = tf.stack(labels)\n",
    "\n",
    "# transform into a keras dataset\n",
    "data = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "data_batch = data.batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 1s 1ms/step - loss: 2.3881\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3570\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 2.3299\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3142\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3071\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3042\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3029\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3023\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3020\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 0s 1ms/step - loss: 2.3017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1331f4400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model in 4 epochs\n",
    "m.fit(data_batch, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding**: We find by the loss function that the model trains nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
